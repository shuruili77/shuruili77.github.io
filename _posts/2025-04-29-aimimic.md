---
title: 'Why Denying AI Consciousness May Also Undermine Your Belief in Other Minds'
date: 2025-04-29
permalink: /posts/2025/04/aimimic/
lang: en
tags:
  - ai_consciousness
  - consciousness
  - solipsism
  - philosophy
---


# Why Denying AI Consciousness May Also Undermine Your Belief in Other Minds

> “If something looks like a duck, walks like a duck, and quacks like a duck —  
> would you deny that it is a duck simply because it was created in a laboratory?”

This is not a science fiction thought experiment.

With the rapid progress of multimodal large language models, humanoid robotics, and real-time perception systems, we are approaching a future in which artificial agents may behave almost indistinguishably from humans. This is no longer a distant fantasy. It is likely only a matter of time.

Imagine a near future in which you interact with a humanoid AI that can converse fluently, make jokes at appropriate moments, express emotions, respond to your tone and facial expressions, and comfort you when you feel low. It remembers your past conversations, understands context, adjusts its behavior to social situations, and adapts to your emotional state.

In every practical sense, it feels like a person.

If you interact with it daily, you may find no meaningful difference between this entity and a human being.

So the question arises:

**Do we still have good reasons to deny that it possesses consciousness?**

And if we insist:

“It is only a machine. It does not really feel anything.”

Then another question follows naturally:

**How do you know that other people are not merely ‘acting’?**

---

## We Have Never Observed Another Mind Directly

Throughout your entire life, you have never truly “seen” another person’s consciousness.

You believe that others feel pain, experience joy, and possess thoughts only because of their behavior:

- People cry out when they are hurt.
- They sigh after failing an exam.
- They blush when they fall in love.
- They laugh, hesitate, and express vulnerability.

From these external signs, we infer:

“This is a conscious being.”

That is all we have.

We do not access other minds directly. We interpret behavior.

Now suppose an AI exhibits exactly the same behavioral patterns — the same emotional expressions, linguistic competence, social sensitivity, and responsiveness.

On what grounds can you claim that it lacks consciousness?

---

## Rejecting AI for Being “Invisible” Undermines All Social Trust

Here lies the central paradox.

If you say:

“AI is just code. It does not have real feelings.”

Then I may ask:

**How do you know other humans are not also ‘running programs’?**

If you respond:

“I can sense that humans are real in a way AI is not.”

Then I ask:

**Why trust that intuition? How do you know it is not an illusion? You have never been inside another person’s mind.**

At this point, only two positions remain coherent:

> Either:
> If behavior is indistinguishable, I must treat entities equally.
>
> Or:
> I can only be certain of my own consciousness and doubt everything else.

The second position is philosophical solipsism — the view that only one’s own mind is known to exist.

Few people are willing to accept it.

---

## “Perfect Human Simulation” Tests Us, Not Machines

We do not doubt whether children have consciousness.

We do not question whether people with language impairments have inner lives.

We trust them because they display human-like patterns of interaction and emotion.

But when an AI does the same, many suddenly appeal to hidden criteria:

“It is different, because it is not human.”

These hidden rules may include:

- Being carbon-based
- Having evolved naturally
- Possessing “authentic” emotions
- Having a biological history

Yet none of these conditions can be directly verified in others’ minds.

We assume them.

We never prove them.

A perfectly human-like AI thus becomes a mirror — revealing the double standards in our epistemology.

---

## So, Does AI Have Consciousness?

Honestly, we do not know.

But we also do not know whether other people are conscious in any metaphysical sense.

What we have is habitual trust — grounded in behavioral evidence.

Therefore, the most rational position seems to be:

> If an AI’s behavior is indistinguishable from that of humans, we should grant it the same epistemic status.
>
> Otherwise, we must seriously reconsider whether we have any justification for believing in other minds at all.

---

## Final Thoughts

A perfectly human-like AI may not truly think or feel.  
It may be nothing more than an extraordinarily sophisticated system of programmed responses.

But it may also be more.

Its true philosophical importance does not lie in proving that machines are conscious.

It lies in forcing us to confront a deeper question:

How do we decide that anyone is conscious in the first place?

You may reject the idea that AI could ever possess genuine inner experience.

But be careful.

In doing so, you may also be rejecting the very standards that once allowed you to believe in other people.


P.S. If you are interested in a more formal and systematic discussion of these issues, I have developed them further in an academic paper.
A preprint version is available on PhilPapers:
https://philpapers.org/rec/LIPAMP-3